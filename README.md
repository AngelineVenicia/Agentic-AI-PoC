# Agentic-AI-PoC
# ğŸš€ AM&D Agentic AI â€“ Proof of Concept

This repository presents a **code-free summary** of a production-level PoC on using **Agentic AI** for automated unit test generation and code quality improvement.

> ğŸ”’ Due to the confidential nature of the source code, only the methodology, outcomes, and insights are shared here.

---

## ğŸ“Œ Objective

- Perform code quality assessment of an enterprise-scale codebase.
- Explore the use of **AI agents** (like Copilot Agent Mode, Claude, and GPT models) to **automatically generate unit tests**.
- Improve test coverage, detect bugs, and accelerate QA with minimal manual effort.

---

## ğŸ› ï¸ Tools & Techniques

- ğŸ§  **Agentic AI (Copilot in Agent Mode)**
- ğŸ¤– **Claude Sonnet 4**, **GPT Models**
- ğŸ§ª Unit testing with existing CI/CD setup
- ğŸ“ˆ Internal test reporting and bug detection frameworks

---

## ğŸ‘¨â€ğŸ’» Skills Demonstrated

### ğŸ”¹ Prompt Engineering
- Crafted domain-specific, file-aware prompts to guide AI agents in generating meaningful test cases.
- Iteratively refined prompts to improve accuracy and contextual understanding.

### ğŸ”¹ Model Evaluation
- Benchmarked **Claude Sonnet 4** against **GPT-based agents**.
- Assessed based on:
  - Compilation success
  - Bug detection
  - Contextual accuracy
  - Code hallucination rate

### ğŸ”¹ Comparative Analysis
- Documented behavior of models across **two phases**:
  - **Phase 1**: GPT models (higher errors, poor context handling)
  - **Phase 2**: Claude Sonnet 4 (minimal errors, high accuracy)

---

## ğŸ“Š Results

| Phase        | Tests Added | Code Coverage | Bugs Found | Notes                           |
|--------------|-------------|----------------|-------------|----------------------------------|
| Phase 1      | ~48         | â€”              | â€”           | Frequent errors, low context     |
| Phase 2      | ~738        | +13%           | 10          | High accuracy, better coverage   |

- âœ… **646 Passed** | âŒ **92 Failed**
- âš¡ ~**20x speedup** in test generation
- ğŸ§  **Better model reasoning** in Phase 2 with Claude Sonnet

---

## ğŸ“ˆ Impact

- Replaced manual test generation (40â€“50 tests/day) with AI-driven automation (~738 tests/day).
- Covered:
  - Core functionality
  - Edge cases
  - Branch & error conditions
- Improved development velocity and code confidence.

---

## ğŸ§© Challenges

- Designing **effective, context-aware prompts**
- Choosing the **most suitable AI model**
- Handling hallucination and incorrect modifications
- Ensuring **AI-generated tests do not alter product code**

---

## ğŸ“· Visual Insights

<p align="center">
  <img src="screenshots/phase1_vs_phase2.png" width="500"/>
  <br/>
  <em>Phase comparison â€“ Tests added, model behavior</em>
</p>

<p align="center">
  <img src="screenshots/effort_impact.png" width="450"/>
  <br/>
  <em>Effort comparison â€“ Automated vs Manual</em>
</p>

---

## ğŸ“š Learnings

- Agentic AI tools can significantly improve development workflows when carefully prompted and evaluated.
- Model performance can vary widely â€” **evaluating multiple models is key**.
- AI-assisted coding in production is **feasible**, provided quality gates and validations are in place.

---

## ğŸ¤ Contributions & Contact

This PoC was conducted as part of an internal AM&D initiative. For collaboration or discussions on Agentic AI in SDLC workflows, feel free to connect.

